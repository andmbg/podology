{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T15:11:11.458476Z",
     "start_time": "2025-03-30T15:11:11.410725Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "os.environ[\"TEST\"] = \"False\"\n",
    "os.chdir(Path.cwd().parent)\n",
    "import json\n",
    "import sqlite3\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "from podology.data.Episode import Episode\n",
    "from podology.data.EpisodeStore import EpisodeStore\n",
    "from podology.data.Transcript import Transcript\n",
    "from podology.search.utils import make_index_name\n",
    "\n",
    "from config import DB_PATH\n",
    "\n",
    "# set cwd one level up:\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T15:11:24.082360Z",
     "start_time": "2025-03-30T15:11:24.055083Z"
    }
   },
   "outputs": [],
   "source": [
    "episode_store = EpisodeStore()\n",
    "ep = episode_store[\"2KGeX\"]\n",
    "t = Transcript(ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_client = Elasticsearch(\n",
    "    \"http://localhost:9200\",\n",
    "    basic_auth=(os.getenv(\"ELASTIC_USER\"), os.getenv(\"ELASTIC_PASSWORD\")),\n",
    "    # verify_certs=True,\n",
    "    # ca_certs=basedir / \"http_ca.crt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Or your embedding model\n",
    "\n",
    "eid = \"r7Hpj\"\n",
    "term = \"trying a new biscuit variety\"\n",
    "search_embedding = model.encode(term).tolist()\n",
    "search_embedding[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector similarity search\n",
    "vector_query = {\n",
    "    \"query\": {\n",
    "        \"bool\": {\n",
    "            \"must\": [\n",
    "                {\"match\": {\"eid\": eid}}\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    \"knn\": {\n",
    "        \"field\": \"embedding\",\n",
    "        \"query_vector\": search_embedding,\n",
    "        \"k\": 1000,\n",
    "        \"num_candidates\": 1000,\n",
    "        \"filter\": {\"term\": {\"eid\": eid}}\n",
    "    },\n",
    "    \"_source\": [\"eid\", \"text\", \"start\", \"end\", \"title\"],\n",
    "    \"size\": 1000\n",
    "}\n",
    "\n",
    "response = es_client.search(index=\"test_chunks\", body=vector_query)\n",
    "\n",
    "chunk_similarities = [\n",
    "    {\n",
    "        \"start\": hit[\"_source\"][\"start\"],\n",
    "        \"end\": hit[\"_source\"][\"end\"],\n",
    "        \"similarity_score\": hit[\"_score\"],\n",
    "    }\n",
    "    for hit in response[\"hits\"][\"hits\"]\n",
    "]\n",
    "\n",
    "relevance_df = pd.DataFrame(chunk_similarities).sort_values(\"start\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_relevance_scores(relevance_df, ep_duration, n_bins=500):\n",
    "    \"\"\"\n",
    "    Bin relevance scores into time-based bins, averaging overlapping chunks.\n",
    "    \"\"\"\n",
    "    if relevance_df.empty:\n",
    "        return pd.DataFrame({'bin_start': [], 'bin_end': [], 'avg_similarity': []})\n",
    "    \n",
    "    # Get the total time span\n",
    "    min_time = 0\n",
    "    max_time = ep_duration\n",
    "    \n",
    "    # Create bin edges\n",
    "    bin_edges = np.linspace(min_time, max_time, n_bins + 1)\n",
    "    \n",
    "    # Calculate bin centers and create result dataframe\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "    bin_starts = bin_edges[:-1]\n",
    "    bin_ends = bin_edges[1:]\n",
    "    \n",
    "    binned_scores = []\n",
    "    \n",
    "    for i in range(n_bins):\n",
    "        bin_start = bin_edges[i]\n",
    "        bin_end = bin_edges[i + 1]\n",
    "        \n",
    "        # Find chunks that overlap with this bin\n",
    "        overlapping_chunks = relevance_df[\n",
    "            (relevance_df['start'] < bin_end) & (relevance_df['end'] > bin_start)\n",
    "        ]\n",
    "        \n",
    "        if len(overlapping_chunks) > 0:\n",
    "            # Calculate overlap weights for averaging\n",
    "            weighted_scores = []\n",
    "            total_weight = 0\n",
    "            \n",
    "            for _, chunk in overlapping_chunks.iterrows():\n",
    "                # Calculate overlap duration\n",
    "                overlap_start = max(chunk['start'], bin_start)\n",
    "                overlap_end = min(chunk['end'], bin_end)\n",
    "                overlap_duration = overlap_end - overlap_start\n",
    "                \n",
    "                if overlap_duration > 0:\n",
    "                    # Weight by overlap duration\n",
    "                    weighted_scores.append(chunk['similarity_score'] * overlap_duration)\n",
    "                    total_weight += overlap_duration\n",
    "            \n",
    "            if total_weight > 0:\n",
    "                avg_score = sum(weighted_scores) / total_weight\n",
    "            else:\n",
    "                avg_score = overlapping_chunks['similarity_score'].mean()\n",
    "        else:\n",
    "            # No chunks in this bin\n",
    "            avg_score = 0.0\n",
    "        \n",
    "        binned_scores.append(avg_score)\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'bin_start': bin_starts,\n",
    "        'bin_end': bin_ends,\n",
    "        'bin_center': bin_centers,\n",
    "        'avg_similarity': binned_scores\n",
    "    })\n",
    "\n",
    "binned_relevance = bin_relevance_scores(relevance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_relevance.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_chunk_status_issue():\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    \n",
    "    cur = conn.execute(\"SELECT * FROM episodes LIMIT 0\")\n",
    "    actual_columns = [desc[0] for desc in cur.description]\n",
    "    print(f\"  Columns: {actual_columns}\")\n",
    "\n",
    "debug_chunk_status_issue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(DB_PATH)\n",
    "cur = conn.execute(\"SELECT * FROM episodes\")\n",
    "cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# migration_script.py\n",
    "import sqlite3\n",
    "from config import DB_PATH\n",
    "\n",
    "def migrate_duration_column():\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    \n",
    "    try:\n",
    "        # Check current schema\n",
    "        cur = conn.execute(\"PRAGMA table_info(episodes)\")\n",
    "        columns = cur.fetchall()\n",
    "        print(\"Current schema:\")\n",
    "        for col in columns:\n",
    "            print(f\"  {col[1]}: {col[2]}\")\n",
    "        \n",
    "        # Perform migration\n",
    "        print(\"Starting migration...\")\n",
    "        \n",
    "        conn.execute(\"BEGIN TRANSACTION\")\n",
    "        \n",
    "        # Your migration code here (use Method 2 approach)\n",
    "        conn.execute(\"\"\"\n",
    "            CREATE TABLE episodes_new (\n",
    "                eid TEXT PRIMARY KEY,\n",
    "                url TEXT UNIQUE,\n",
    "                title TEXT,\n",
    "                pub_date TEXT,\n",
    "                description TEXT,\n",
    "                duration FLOAT,\n",
    "                transcript_status TEXT,\n",
    "                transcript_wcstatus TEXT,\n",
    "                audio_status TEXT,\n",
    "                chunk_status TEXT\n",
    "            )\n",
    "        \"\"\")\n",
    "        \n",
    "        conn.execute(\"\"\"\n",
    "            INSERT INTO episodes_new \n",
    "            SELECT \n",
    "                eid, url, title, pub_date, description,\n",
    "                CAST(duration AS FLOAT) as duration,\n",
    "                transcript_status, transcript_wcstatus, \n",
    "                audio_status, chunk_status\n",
    "            FROM episodes\n",
    "        \"\"\")\n",
    "        \n",
    "        conn.execute(\"DROP TABLE episodes\")\n",
    "        conn.execute(\"ALTER TABLE episodes_new RENAME TO episodes\")\n",
    "        \n",
    "        conn.execute(\"COMMIT\")\n",
    "        print(\"Migration completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        conn.execute(\"ROLLBACK\")\n",
    "        print(f\"Migration failed: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    migrate_duration_column()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def nuclear_option_rebuild_database():\n",
    "    \"\"\"Completely rebuild the database from scratch\"\"\"\n",
    "    import shutil\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # Backup the current database\n",
    "    backup_path = f\"{DB_PATH}.backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    shutil.copy2(DB_PATH, backup_path)\n",
    "    print(f\"Backed up database to: {backup_path}\")\n",
    "    \n",
    "    # Get all the data first\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    cur = conn.execute(\"SELECT * FROM episodes\")\n",
    "    all_data = cur.fetchall()\n",
    "    \n",
    "    # Get just the columns that work\n",
    "    cur = conn.execute(\"SELECT eid, url, title, pub_date, description, duration, transcript_status, transcript_wcstatus, audio_status FROM episodes\")\n",
    "    working_data = cur.fetchall()\n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"Extracted {len(working_data)} rows\")\n",
    "    \n",
    "    # Delete the database file\n",
    "    os.remove(DB_PATH)\n",
    "    print(f\"Deleted {DB_PATH}\")\n",
    "    \n",
    "    # Create fresh database\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    \n",
    "    # Create table with correct schema\n",
    "    conn.execute(\"\"\"\n",
    "        CREATE TABLE episodes (\n",
    "            eid TEXT PRIMARY KEY,\n",
    "            url TEXT UNIQUE,\n",
    "            title TEXT,\n",
    "            pub_date TEXT,\n",
    "            description TEXT,\n",
    "            duration FLOAT,\n",
    "            transcript_status TEXT,\n",
    "            transcript_wcstatus TEXT,\n",
    "            audio_status TEXT,\n",
    "            chunk_status TEXT\n",
    "        )\n",
    "    \"\"\")\n",
    "    \n",
    "    # Insert all data\n",
    "    for row in working_data:\n",
    "        eid, url, title, pub_date, description, duration, transcript_status, transcript_wcstatus, audio_status = row\n",
    "        \n",
    "        # Convert duration\n",
    "        try:\n",
    "            duration_float = float(duration) if duration is not None else None\n",
    "        except:\n",
    "            duration_float = None\n",
    "            \n",
    "        conn.execute(\"\"\"\n",
    "            INSERT INTO episodes \n",
    "            (eid, url, title, pub_date, description, duration, transcript_status, transcript_wcstatus, audio_status, chunk_status)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        \"\"\", (eid, url, title, pub_date, description, duration_float, transcript_status, transcript_wcstatus, audio_status, None))\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "    print(\"Database rebuilt successfully!\")\n",
    "    \n",
    "    # Test the new database\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    cur = conn.execute(\"PRAGMA table_info(episodes)\")\n",
    "    columns = cur.fetchall()\n",
    "    print(\"Final schema:\")\n",
    "    for col in columns:\n",
    "        print(f\"  {col[1]}: {col[2]}\")\n",
    "    conn.close()\n",
    "\n",
    "nuclear_option_rebuild_database()  # Uncomment to run"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "podology-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
